{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cursor Conversation Exporter\n",
    "\n",
    "Export your Cursor AI chat history to a shareable JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T20:52:19.906364Z",
     "iopub.status.busy": "2025-10-30T20:52:19.906092Z",
     "iopub.status.idle": "2025-10-30T20:52:20.206315Z",
     "shell.execute_reply": "2025-10-30T20:52:20.205755Z"
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T20:52:20.207519Z",
     "iopub.status.busy": "2025-10-30T20:52:20.207415Z",
     "iopub.status.idle": "2025-10-30T20:52:20.209548Z",
     "shell.execute_reply": "2025-10-30T20:52:20.209248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Database: /Users/shaun/Library/Application Support/Cursor/User/globalStorage/state.vscdb\n",
      "✅ Exists: True\n"
     ]
    }
   ],
   "source": [
    "# Connect to Cursor database\n",
    "db_path = os.path.expanduser('~/Library/Application Support/Cursor/User/globalStorage/state.vscdb')\n",
    "print(f\"📂 Database: {db_path}\")\n",
    "print(f\"✅ Exists: {os.path.exists(db_path)}\")\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T20:52:20.227971Z",
     "iopub.status.busy": "2025-10-30T20:52:20.227877Z",
     "iopub.status.idle": "2025-10-30T20:52:20.229525Z",
     "shell.execute_reply": "2025-10-30T20:52:20.229117Z"
    }
   },
   "outputs": [],
   "source": [
    "# Placeholder - will add metadata exploration after conversations are loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T20:52:20.230664Z",
     "iopub.status.busy": "2025-10-30T20:52:20.230598Z",
     "iopub.status.idle": "2025-10-30T20:52:20.615459Z",
     "shell.execute_reply": "2025-10-30T20:52:20.615046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading conversations from database...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 17,363 message bubbles\n"
     ]
    }
   ],
   "source": [
    "# Load all conversation bubbles\n",
    "print(\"Loading conversations from database...\")\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT key, value \n",
    "    FROM cursorDiskKV \n",
    "    WHERE key LIKE 'bubbleId:%'\n",
    "\"\"\")\n",
    "\n",
    "conversations_raw = cursor.fetchall()\n",
    "print(f\"✅ Loaded {len(conversations_raw):,} message bubbles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T20:52:20.616544Z",
     "iopub.status.busy": "2025-10-30T20:52:20.616478Z",
     "iopub.status.idle": "2025-10-30T20:52:20.997357Z",
     "shell.execute_reply": "2025-10-30T20:52:20.997027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXPLORING MESSAGE METADATA\n",
      "================================================================================\n",
      "\n",
      "🔍 Examining 5 sample messages:\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 1: USER message (tokens: False)\n",
      "================================================================================\n",
      "\n",
      "📋 All available fields (70 total):\n",
      "   _v, aiWebSearchResults, allThinkingBlocks, approximateLintErrors, assistantSuggestedDiffs, attachedCodeChunks, attachedFileCodeChunksMetadataOnly, attachedFolders, attachedFoldersListDirResults, attachedFoldersNew, attachedHumanChanges, bubbleId, capabilities, capabilityContexts, capabilityStatuses, checkpointId, codebaseContextChunks, commits, consoleLogs, context, contextPieces, createdAt, cursorRules, deletedFiles, diffHistories, diffsForCompressingFiles, diffsSinceLastApply, docsReferences, documentationSelections, editToolSupportsSearchAndReplace, editTrailContexts, existedPreviousTerminalCommand, existedSubsequentTerminalCommand, externalLinks, fileDiffTrajectories, gitDiffs, humanChanges, images, interpreterResults, isAgentic, isNudge, isPlanExecution, isQuickSearchQuery, isRefunded, knowledgeItems, lints, modelInfo, multiFileLinterErrors, notepads, projectLayouts, pullRequests, recentLocationsHistory, recentlyViewedFiles, relevantFiles, requestId, richText, skipRendering, suggestedCodeBlocks, summarizedComposers, supportedTools, text, todos, tokenCount, toolResults, type, uiElementPicked, unifiedMode, useWeb, userResponsesToSuggestedCodeBlocks, webReferences\n",
      "\n",
      "📊 Key metadata:\n",
      "   • type: 1 (user)\n",
      "   • createdAt: 2025-10-13T00:42:12.260Z\n",
      "   • text length: 173 chars\n",
      "   • modelInfo: {'modelName': 'default'}\n",
      "   • tokenCount: input=0, output=0\n",
      "      All tokenCount fields: ['inputTokens', 'outputTokens']\n",
      "   • context type: dict, keys: ['notepads', 'composers', 'quotes', 'selectedCommits', 'selectedPullRequests', 'selectedImages', 'folderSelections', 'fileSelections', 'terminalFiles', 'selections', 'terminalSelections', 'selectedDocs', 'externalLinks', 'cursorRules', 'cursorCommands', 'uiElementSelections', 'consoleLogs', 'mentions']\n",
      "   • isAgentic: True\n",
      "   • requestId: 59bf0b3c-9525-478e-801c-8c5e82ed4c74...\n",
      "\n",
      "💬 Text preview:\n",
      "   Run these for me\n",
      "\n",
      "echo >> /Users/shaun/.zprofile\n",
      "    echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"' >> /Users/shaun/.zprofile\n",
      "    eval \"$(/opt/homebrew/bin/brew shellenv)\"\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 2: ASSISTANT message (tokens: False)\n",
      "================================================================================\n",
      "\n",
      "📋 All available fields (70 total):\n",
      "   _v, aiWebSearchResults, allThinkingBlocks, approximateLintErrors, assistantSuggestedDiffs, attachedCodeChunks, attachedFileCodeChunksMetadataOnly, attachedFolders, attachedFoldersListDirResults, attachedFoldersNew, attachedHumanChanges, bubbleId, capabilities, capabilityContexts, capabilityStatuses, capabilityType, codeBlocks, codebaseContextChunks, commits, consoleLogs, contextPieces, createdAt, cursorRules, deletedFiles, diffHistories, diffsForCompressingFiles, diffsSinceLastApply, docsReferences, documentationSelections, editTrailContexts, existedPreviousTerminalCommand, existedSubsequentTerminalCommand, externalLinks, fileDiffTrajectories, gitDiffs, humanChanges, images, interpreterResults, isAgentic, isQuickSearchQuery, isRefunded, knowledgeItems, lints, modelInfo, multiFileLinterErrors, notepads, projectLayouts, pullRequests, recentLocationsHistory, recentlyViewedFiles, relevantFiles, requestId, serverBubbleId, suggestedCodeBlocks, summarizedComposers, supportedTools, text, thinking, thinkingDurationMs, timingInfo, todos, tokenCount, toolResults, type, uiElementPicked, unifiedMode, usageUuid, useWeb, userResponsesToSuggestedCodeBlocks, webReferences\n",
      "\n",
      "📊 Key metadata:\n",
      "   • type: 2 (assistant)\n",
      "   • createdAt: 2025-10-13T00:42:12.296Z\n",
      "   • text length: 0 chars\n",
      "   • modelInfo: {'modelName': 'default'}\n",
      "   • tokenCount: input=0, output=0\n",
      "      All tokenCount fields: ['inputTokens', 'outputTokens']\n",
      "   • requestId: 59bf0b3c-9525-478e-801c-8c5e82ed4c74...\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 3: ASSISTANT message (tokens: False)\n",
      "================================================================================\n",
      "\n",
      "📋 All available fields (63 total):\n",
      "   _v, aiWebSearchResults, allThinkingBlocks, approximateLintErrors, assistantSuggestedDiffs, attachedCodeChunks, attachedFileCodeChunksMetadataOnly, attachedFolders, attachedFoldersListDirResults, attachedFoldersNew, attachedHumanChanges, bubbleId, capabilities, capabilityContexts, capabilityStatuses, codebaseContextChunks, commits, consoleLogs, contextPieces, createdAt, cursorRules, deletedFiles, diffHistories, diffsForCompressingFiles, diffsSinceLastApply, docsReferences, documentationSelections, editTrailContexts, existedPreviousTerminalCommand, existedSubsequentTerminalCommand, externalLinks, fileDiffTrajectories, gitDiffs, humanChanges, images, interpreterResults, isAgentic, isQuickSearchQuery, isRefunded, knowledgeItems, lints, multiFileLinterErrors, notepads, projectLayouts, pullRequests, recentLocationsHistory, recentlyViewedFiles, relevantFiles, requestId, suggestedCodeBlocks, summarizedComposers, supportedTools, text, todos, tokenCount, toolResults, type, uiElementPicked, unifiedMode, usageUuid, useWeb, userResponsesToSuggestedCodeBlocks, webReferences\n",
      "\n",
      "📊 Key metadata:\n",
      "   • type: 2 (assistant)\n",
      "   • createdAt: 2025-10-13T00:42:19.925Z\n",
      "   • text length: 65 chars\n",
      "   • tokenCount: input=0, output=0\n",
      "      All tokenCount fields: ['inputTokens', 'outputTokens']\n",
      "\n",
      "💬 Text preview:\n",
      "   I'll run these commands to set up Homebrew in your shell profile.\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 4: ASSISTANT message (tokens: False)\n",
      "================================================================================\n",
      "\n",
      "📋 All available fields (65 total):\n",
      "   _v, aiWebSearchResults, allThinkingBlocks, approximateLintErrors, assistantSuggestedDiffs, attachedCodeChunks, attachedFileCodeChunksMetadataOnly, attachedFolders, attachedFoldersListDirResults, attachedFoldersNew, attachedHumanChanges, bubbleId, capabilities, capabilityContexts, capabilityStatuses, capabilityType, codeBlocks, codebaseContextChunks, commits, consoleLogs, contextPieces, createdAt, cursorRules, deletedFiles, diffHistories, diffsForCompressingFiles, diffsSinceLastApply, docsReferences, documentationSelections, editTrailContexts, existedPreviousTerminalCommand, existedSubsequentTerminalCommand, externalLinks, fileDiffTrajectories, gitDiffs, humanChanges, images, interpreterResults, isAgentic, isQuickSearchQuery, isRefunded, knowledgeItems, lints, multiFileLinterErrors, notepads, projectLayouts, pullRequests, recentLocationsHistory, recentlyViewedFiles, relevantFiles, requestId, suggestedCodeBlocks, summarizedComposers, supportedTools, text, todos, tokenCount, toolFormerData, toolResults, type, uiElementPicked, unifiedMode, useWeb, userResponsesToSuggestedCodeBlocks, webReferences\n",
      "\n",
      "📊 Key metadata:\n",
      "   • type: 2 (assistant)\n",
      "   • createdAt: 2025-10-13T00:42:20.358Z\n",
      "   • text length: 0 chars\n",
      "   • tokenCount: input=0, output=0\n",
      "      All tokenCount fields: ['inputTokens', 'outputTokens']\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 5: ASSISTANT message (tokens: False)\n",
      "================================================================================\n",
      "\n",
      "📋 All available fields (65 total):\n",
      "   _v, aiWebSearchResults, allThinkingBlocks, approximateLintErrors, assistantSuggestedDiffs, attachedCodeChunks, attachedFileCodeChunksMetadataOnly, attachedFolders, attachedFoldersListDirResults, attachedFoldersNew, attachedHumanChanges, bubbleId, capabilities, capabilityContexts, capabilityStatuses, capabilityType, codeBlocks, codebaseContextChunks, commits, consoleLogs, contextPieces, createdAt, cursorRules, deletedFiles, diffHistories, diffsForCompressingFiles, diffsSinceLastApply, docsReferences, documentationSelections, editTrailContexts, existedPreviousTerminalCommand, existedSubsequentTerminalCommand, externalLinks, fileDiffTrajectories, gitDiffs, humanChanges, images, interpreterResults, isAgentic, isQuickSearchQuery, isRefunded, knowledgeItems, lints, multiFileLinterErrors, notepads, projectLayouts, pullRequests, recentLocationsHistory, recentlyViewedFiles, relevantFiles, requestId, suggestedCodeBlocks, summarizedComposers, supportedTools, text, todos, tokenCount, toolFormerData, toolResults, type, uiElementPicked, unifiedMode, useWeb, userResponsesToSuggestedCodeBlocks, webReferences\n",
      "\n",
      "📊 Key metadata:\n",
      "   • type: 2 (assistant)\n",
      "   • createdAt: 2025-10-13T00:42:20.358Z\n",
      "   • text length: 0 chars\n",
      "   • tokenCount: input=0, output=0\n",
      "      All tokenCount fields: ['inputTokens', 'outputTokens']\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ANALYZING ALL MESSAGES FOR PATTERNS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Message Type Distribution:\n",
      "   • user: 742 (4.3%)\n",
      "   • assistant: 0 (0.0%)\n",
      "   • other: 16,621 (95.7%)\n",
      "\n",
      "🪙 Token Distribution:\n",
      "   • with_tokens: 619 (3.6%)\n",
      "   • without_tokens: 16,744 (96.4%)\n",
      "\n",
      "🤖 Model Usage:\n",
      "   • claude-4.5-sonnet-thinking: 1,151\n",
      "   • cheetah: 208\n",
      "   • default: 76\n",
      "   • gpt-5: 30\n",
      "   • composer-1: 24\n",
      "\n",
      "🛠️  Most Common Capabilities:\n",
      "\n",
      "🧠 Special Features:\n",
      "   • Messages with thinking blocks: 0\n",
      "   • Agentic messages: 721\n",
      "\n",
      "📋 Most Populated Fields (non-empty):\n",
      "   • _v: 17,363 (100.0%)\n",
      "   • type: 17,363 (100.0%)\n",
      "   • bubbleId: 17,363 (100.0%)\n",
      "   • capabilityStatuses: 17,363 (100.0%)\n",
      "   • tokenCount: 17,363 (100.0%)\n",
      "   • unifiedMode: 17,363 (100.0%)\n",
      "   • createdAt: 17,363 (100.0%)\n",
      "   • capabilityType: 12,225 (70.4%)\n",
      "   • toolFormerData: 12,043 (69.4%)\n",
      "   • usageUuid: 10,896 (62.8%)\n",
      "   • serverBubbleId: 6,107 (35.2%)\n",
      "   • text: 4,997 (28.8%)\n",
      "   • thinking: 4,823 (27.8%)\n",
      "   • thinkingDurationMs: 4,823 (27.8%)\n",
      "   • checkpointId: 3,178 (18.3%)\n",
      "   • codeBlocks: 2,680 (15.4%)\n",
      "   • requestId: 1,489 (8.6%)\n",
      "   • modelInfo: 1,489 (8.6%)\n",
      "   • timingInfo: 748 (4.3%)\n",
      "   • supportedTools: 742 (4.3%)\n",
      "   • richText: 742 (4.3%)\n",
      "   • editToolSupportsSearchAndReplace: 742 (4.3%)\n",
      "   • context: 723 (4.2%)\n",
      "   • isAgentic: 721 (4.2%)\n",
      "   • contextWindowStatusAtCreation: 589 (3.4%)\n"
     ]
    }
   ],
   "source": [
    "# Deep dive into message metadata\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXPLORING MESSAGE METADATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Let's look at several different messages to understand the patterns\n",
    "sample_messages = []\n",
    "\n",
    "# Get samples of different types\n",
    "for i, (key, value_blob) in enumerate(conversations_raw[:100]):  # Check first 100\n",
    "    try:\n",
    "        data = json.loads(value_blob)\n",
    "        msg_type = data.get('type', 0)\n",
    "        has_text = bool(data.get('text', '').strip())\n",
    "        token_count = data.get('tokenCount', {})\n",
    "        has_tokens = bool(token_count and (token_count.get('inputTokens', 0) > 0 or token_count.get('outputTokens', 0) > 0))\n",
    "        \n",
    "        # Collect different types\n",
    "        if len(sample_messages) < 5:\n",
    "            sample_messages.append({\n",
    "                'index': i,\n",
    "                'key': key,\n",
    "                'type': 'user' if msg_type == 1 else 'assistant',\n",
    "                'has_text': has_text,\n",
    "                'has_tokens': has_tokens,\n",
    "                'data': data\n",
    "            })\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f\"\\n🔍 Examining {len(sample_messages)} sample messages:\\n\")\n",
    "\n",
    "for idx, sample in enumerate(sample_messages, 1):\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"SAMPLE {idx}: {sample['type'].upper()} message (tokens: {sample['has_tokens']})\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    data = sample['data']\n",
    "    \n",
    "    # Show all top-level keys\n",
    "    print(f\"\\n📋 All available fields ({len(data.keys())} total):\")\n",
    "    print(f\"   {', '.join(sorted(data.keys()))}\")\n",
    "    \n",
    "    # Show interesting fields with values\n",
    "    print(f\"\\n📊 Key metadata:\")\n",
    "    print(f\"   • type: {data.get('type')} ({'user' if data.get('type') == 1 else 'assistant'})\")\n",
    "    print(f\"   • createdAt: {data.get('createdAt')}\")\n",
    "    print(f\"   • text length: {len(data.get('text', ''))} chars\")\n",
    "    \n",
    "    # Model info\n",
    "    if data.get('modelInfo'):\n",
    "        print(f\"   • modelInfo: {data['modelInfo']}\")\n",
    "    \n",
    "    # Token counts\n",
    "    if data.get('tokenCount'):\n",
    "        tc = data['tokenCount']\n",
    "        print(f\"   • tokenCount: input={tc.get('inputTokens', 0)}, output={tc.get('outputTokens', 0)}\")\n",
    "        print(f\"      All tokenCount fields: {list(tc.keys())}\")\n",
    "    \n",
    "    # Context and capabilities\n",
    "    if data.get('context'):\n",
    "        print(f\"   • context type: {type(data['context']).__name__}, keys: {list(data['context'].keys()) if isinstance(data['context'], dict) else 'N/A'}\")\n",
    "    \n",
    "    if data.get('capabilities'):\n",
    "        print(f\"   • capabilities: {len(data['capabilities'])} items\")\n",
    "        if data['capabilities']:\n",
    "            print(f\"      Sample: {list(data['capabilities'])[:3]}\")\n",
    "    \n",
    "    # Tool usage\n",
    "    if data.get('toolResults'):\n",
    "        print(f\"   • toolResults: {len(data['toolResults'])} results\")\n",
    "    \n",
    "    # Agentic behavior\n",
    "    if data.get('isAgentic'):\n",
    "        print(f\"   • isAgentic: {data['isAgentic']}\")\n",
    "    \n",
    "    # Thinking blocks\n",
    "    if data.get('allThinkingBlocks'):\n",
    "        print(f\"   • allThinkingBlocks: {len(data['allThinkingBlocks'])} blocks\")\n",
    "    \n",
    "    # Request ID (useful for grouping)\n",
    "    if data.get('requestId'):\n",
    "        print(f\"   • requestId: {data['requestId'][:40]}...\")\n",
    "    \n",
    "    # Show sample text\n",
    "    text = data.get('text', '')\n",
    "    if text:\n",
    "        print(f\"\\n💬 Text preview:\")\n",
    "        print(f\"   {text[:200]}\")\n",
    "        if len(text) > 200:\n",
    "            print(\"   ...\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Now let's analyze patterns across ALL messages\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYZING ALL MESSAGES FOR PATTERNS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "field_stats = defaultdict(int)\n",
    "type_stats = {'user': 0, 'assistant': 0, 'other': 0}\n",
    "token_stats = {'with_tokens': 0, 'without_tokens': 0}\n",
    "model_usage = defaultdict(int)\n",
    "capability_usage = defaultdict(int)\n",
    "has_thinking = 0\n",
    "is_agentic = 0\n",
    "\n",
    "for key, value_blob in conversations_raw:\n",
    "    try:\n",
    "        data = json.loads(value_blob)\n",
    "        \n",
    "        # Type\n",
    "        msg_type = data.get('type', 0)\n",
    "        if msg_type == 1:\n",
    "            type_stats['user'] += 1\n",
    "        elif msg_type == 0:\n",
    "            type_stats['assistant'] += 1\n",
    "        else:\n",
    "            type_stats['other'] += 1\n",
    "        \n",
    "        # Tokens\n",
    "        token_count = data.get('tokenCount', {})\n",
    "        if token_count and (token_count.get('inputTokens', 0) > 0 or token_count.get('outputTokens', 0) > 0):\n",
    "            token_stats['with_tokens'] += 1\n",
    "        else:\n",
    "            token_stats['without_tokens'] += 1\n",
    "        \n",
    "        # Model\n",
    "        model_info = data.get('modelInfo', {})\n",
    "        if model_info:\n",
    "            model_name = model_info.get('modelName', 'unknown')\n",
    "            model_usage[model_name] += 1\n",
    "        \n",
    "        # Capabilities\n",
    "        capabilities = data.get('capabilities', [])\n",
    "        for cap in capabilities:\n",
    "            capability_usage[cap] += 1\n",
    "        \n",
    "        # Thinking\n",
    "        if data.get('allThinkingBlocks'):\n",
    "            has_thinking += 1\n",
    "        \n",
    "        # Agentic\n",
    "        if data.get('isAgentic'):\n",
    "            is_agentic += 1\n",
    "        \n",
    "        # Track which fields have non-empty values\n",
    "        for field, value in data.items():\n",
    "            if value:  # Non-empty\n",
    "                if isinstance(value, (list, dict)):\n",
    "                    if len(value) > 0:\n",
    "                        field_stats[field] += 1\n",
    "                else:\n",
    "                    field_stats[field] += 1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f\"\\n📊 Message Type Distribution:\")\n",
    "for msg_type, count in type_stats.items():\n",
    "    print(f\"   • {msg_type}: {count:,} ({count/len(conversations_raw)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n🪙 Token Distribution:\")\n",
    "for stat_type, count in token_stats.items():\n",
    "    print(f\"   • {stat_type}: {count:,} ({count/len(conversations_raw)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n🤖 Model Usage:\")\n",
    "for model, count in sorted(model_usage.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"   • {model}: {count:,}\")\n",
    "\n",
    "print(f\"\\n🛠️  Most Common Capabilities:\")\n",
    "for cap, count in sorted(capability_usage.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"   • {cap}: {count:,}\")\n",
    "\n",
    "print(f\"\\n🧠 Special Features:\")\n",
    "print(f\"   • Messages with thinking blocks: {has_thinking:,}\")\n",
    "print(f\"   • Agentic messages: {is_agentic:,}\")\n",
    "\n",
    "print(f\"\\n📋 Most Populated Fields (non-empty):\")\n",
    "for field, count in sorted(field_stats.items(), key=lambda x: x[1], reverse=True)[:25]:\n",
    "    print(f\"   • {field}: {count:,} ({count/len(conversations_raw)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T20:52:20.998619Z",
     "iopub.status.busy": "2025-10-30T20:52:20.998533Z",
     "iopub.status.idle": "2025-10-30T20:52:21.303058Z",
     "shell.execute_reply": "2025-10-30T20:52:21.302682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing and grouping messages...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Grouped into 151 conversation threads"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Total messages: 4,997\n",
      "📊 Average messages per thread: 33.1\n",
      "\n",
      "🔍 Token Analysis:\n",
      "   • Messages WITH tokens: 602\n",
      "   • Messages WITHOUT tokens: 4,395\n",
      "   • 88.0% have zero tokens\n"
     ]
    }
   ],
   "source": [
    "# Parse and group messages by conversation thread\n",
    "print(\"\\nParsing and grouping messages...\")\n",
    "\n",
    "conversation_threads = defaultdict(list)\n",
    "\n",
    "for key, value_blob in conversations_raw:\n",
    "    try:\n",
    "        data = json.loads(value_blob)\n",
    "        \n",
    "        # Extract IDs from key\n",
    "        parts = key.split(':')\n",
    "        workspace_id = parts[1] if len(parts) > 1 else None\n",
    "        bubble_id = parts[2] if len(parts) > 2 else None\n",
    "        \n",
    "        # Get message details\n",
    "        text = data.get('text', '')\n",
    "        msg_type = data.get('type', 0)\n",
    "        created_at = data.get('createdAt', None)\n",
    "        \n",
    "        # Get token counts and model info - check multiple possible fields\n",
    "        token_count = data.get('tokenCount', {})\n",
    "        model_info = data.get('modelInfo', {})\n",
    "        \n",
    "        # Standard token fields\n",
    "        input_tokens = token_count.get('inputTokens', 0) if token_count else 0\n",
    "        output_tokens = token_count.get('outputTokens', 0) if token_count else 0\n",
    "        \n",
    "        # Check for thinking tokens (newer models may have separate thinking token counts)\n",
    "        thinking_tokens = token_count.get('thinkingTokens', 0) if token_count else 0\n",
    "        \n",
    "        # Also check for alternative field names\n",
    "        if input_tokens == 0 and output_tokens == 0:\n",
    "            input_tokens = data.get('inputTokens', 0)\n",
    "            output_tokens = data.get('outputTokens', 0)\n",
    "            thinking_tokens = data.get('thinkingTokens', 0)\n",
    "        \n",
    "        model_name = model_info.get('modelName', 'unknown') if model_info else 'unknown'\n",
    "        \n",
    "        # Only include messages with actual text\n",
    "        if text and text.strip():\n",
    "            conversation_threads[workspace_id].append({\n",
    "                'bubble_id': bubble_id,\n",
    "                'type': 'user' if msg_type == 1 else 'assistant',\n",
    "                'text': text,\n",
    "                'timestamp': created_at,\n",
    "                'input_tokens': input_tokens,\n",
    "                'output_tokens': output_tokens,\n",
    "                'thinking_tokens': thinking_tokens,\n",
    "                'model': model_name\n",
    "            })\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Sort messages within each thread by timestamp\n",
    "for workspace_id in conversation_threads:\n",
    "    conversation_threads[workspace_id].sort(\n",
    "        key=lambda x: x['timestamp'] if x['timestamp'] else x['bubble_id']\n",
    "    )\n",
    "\n",
    "print(f\"✅ Grouped into {len(conversation_threads)} conversation threads\")\n",
    "total_messages = sum(len(msgs) for msgs in conversation_threads.values())\n",
    "print(f\"📊 Total messages: {total_messages:,}\")\n",
    "print(f\"📊 Average messages per thread: {total_messages / len(conversation_threads):.1f}\")\n",
    "\n",
    "# Debug: Check for messages with zero tokens\n",
    "messages_with_zero_tokens = 0\n",
    "messages_with_tokens = 0\n",
    "for workspace_id, messages in conversation_threads.items():\n",
    "    for msg in messages:\n",
    "        total_tokens = msg.get('input_tokens', 0) + msg.get('output_tokens', 0) + msg.get('thinking_tokens', 0)\n",
    "        if total_tokens == 0:\n",
    "            messages_with_zero_tokens += 1\n",
    "        else:\n",
    "            messages_with_tokens += 1\n",
    "\n",
    "print(f\"\\n🔍 Token Analysis:\")\n",
    "print(f\"   • Messages WITH tokens: {messages_with_tokens:,}\")\n",
    "print(f\"   • Messages WITHOUT tokens: {messages_with_zero_tokens:,}\")\n",
    "if messages_with_zero_tokens > 0:\n",
    "    print(f\"   • {(messages_with_zero_tokens / total_messages * 100):.1f}% have zero tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T20:52:21.304091Z",
     "iopub.status.busy": "2025-10-30T20:52:21.304003Z",
     "iopub.status.idle": "2025-10-30T20:52:21.318042Z",
     "shell.execute_reply": "2025-10-30T20:52:21.317560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating export data structure...\n",
      "✅ Created export structure with 151 conversations\n",
      "🪙 Total tokens: 43,266,408\n",
      "   • Input: 40,709,847\n",
      "   • Output: 2,556,561\n",
      "   • Thinking: 0\n"
     ]
    }
   ],
   "source": [
    "# Create structured export format\n",
    "print(\"\\nCreating export data structure...\")\n",
    "\n",
    "export_data = {\n",
    "    \"metadata\": {\n",
    "        \"export_date\": datetime.now().isoformat(),\n",
    "        \"total_conversations\": len(conversation_threads),\n",
    "        \"total_messages\": sum(len(msgs) for msgs in conversation_threads.values())\n",
    "    },\n",
    "    \"conversations\": []\n",
    "}\n",
    "\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "total_thinking_tokens = 0\n",
    "\n",
    "# Process each conversation thread\n",
    "for workspace_id, messages in conversation_threads.items():\n",
    "    # Find the first user message as the title\n",
    "    title = None\n",
    "    for msg in messages:\n",
    "        if msg['type'] == 'user':\n",
    "            title = msg['text'][:100] + ('...' if len(msg['text']) > 100 else '')\n",
    "            break\n",
    "    \n",
    "    # Count message types and tokens\n",
    "    user_count = sum(1 for m in messages if m['type'] == 'user')\n",
    "    assistant_count = sum(1 for m in messages if m['type'] == 'assistant')\n",
    "    \n",
    "    conv_input_tokens = sum(m.get('input_tokens', 0) for m in messages)\n",
    "    conv_output_tokens = sum(m.get('output_tokens', 0) for m in messages)\n",
    "    conv_thinking_tokens = sum(m.get('thinking_tokens', 0) for m in messages)\n",
    "    \n",
    "    total_input_tokens += conv_input_tokens\n",
    "    total_output_tokens += conv_output_tokens\n",
    "    total_thinking_tokens += conv_thinking_tokens\n",
    "    \n",
    "    # Build conversation object\n",
    "    conversation = {\n",
    "        \"workspace_id\": workspace_id,\n",
    "        \"title\": title or \"(No title)\",\n",
    "        \"message_count\": len(messages),\n",
    "        \"user_messages\": user_count,\n",
    "        \"assistant_messages\": assistant_count,\n",
    "        \"tokens\": {\n",
    "            \"input\": conv_input_tokens,\n",
    "            \"output\": conv_output_tokens,\n",
    "            \"thinking\": conv_thinking_tokens,\n",
    "            \"total\": conv_input_tokens + conv_output_tokens + conv_thinking_tokens\n",
    "        },\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": msg['type'],\n",
    "                \"text\": msg['text'],\n",
    "                \"timestamp\": msg['timestamp'],\n",
    "                \"tokens\": {\n",
    "                    \"input\": msg.get('input_tokens', 0),\n",
    "                    \"output\": msg.get('output_tokens', 0),\n",
    "                    \"thinking\": msg.get('thinking_tokens', 0)\n",
    "                },\n",
    "                \"model\": msg.get('model', 'unknown')\n",
    "            }\n",
    "            for msg in messages\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    export_data[\"conversations\"].append(conversation)\n",
    "\n",
    "# Add global stats\n",
    "export_data[\"metadata\"][\"total_tokens\"] = {\n",
    "    \"input\": total_input_tokens,\n",
    "    \"output\": total_output_tokens,\n",
    "    \"thinking\": total_thinking_tokens,\n",
    "    \"total\": total_input_tokens + total_output_tokens + total_thinking_tokens\n",
    "}\n",
    "\n",
    "# Sort by message count (most active first)\n",
    "export_data[\"conversations\"].sort(key=lambda x: x['message_count'], reverse=True)\n",
    "\n",
    "print(f\"✅ Created export structure with {len(export_data['conversations'])} conversations\")\n",
    "print(f\"🪙 Total tokens: {total_input_tokens + total_output_tokens + total_thinking_tokens:,}\")\n",
    "print(f\"   • Input: {total_input_tokens:,}\")\n",
    "print(f\"   • Output: {total_output_tokens:,}\")\n",
    "print(f\"   • Thinking: {total_thinking_tokens:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T20:52:21.318998Z",
     "iopub.status.busy": "2025-10-30T20:52:21.318926Z",
     "iopub.status.idle": "2025-10-30T20:52:21.321634Z",
     "shell.execute_reply": "2025-10-30T20:52:21.321268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOP 20 CONVERSATIONS (BY MESSAGE COUNT)\n",
      "================================================================================\n",
      "\n",
      "1. I made an .env file - let's set up a basic use-case for Supabase auth \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   💬 198 messages (👤 9 user | 🤖 189 assistant)\n",
      "   🪙 750,580 tokens (in: 670,904, out: 79,676)\n",
      "\n",
      "2. In scripts/cursor_conversations_export.ipynb\n",
      "\n",
      "Let's remove any mention of price, just focus on the t...\n",
      "   💬 178 messages (👤 25 user | 🤖 153 assistant)\n",
      "   🪙 1,853,977 tokens (in: 1,755,565, out: 98,412)\n",
      "\n",
      "3. On the logged in dashboard page, I want to add a \"Connectors\" section and support Google OAuth as a ...\n",
      "   💬 176 messages (👤 15 user | 🤖 161 assistant)\n",
      "   🪙 1,584,741 tokens (in: 1,543,581, out: 41,160)\n",
      "\n",
      "4. I had tried to add a couple of security policies to ensure logged in users could upload images - I t...\n",
      "   💬 161 messages (👤 42 user | 🤖 119 assistant)\n",
      "   🪙 3,838,370 tokens (in: 3,764,047, out: 74,323)\n",
      "\n",
      "5. Can you do an SEO audit and let me know what you find?\n",
      "   💬 160 messages (👤 15 user | 🤖 145 assistant)\n",
      "   🪙 1,139,318 tokens (in: 1,110,940, out: 28,378)\n",
      "\n",
      "6. Hello! Install the GCP CLI and help me log in to it\n",
      "   💬 143 messages (👤 12 user | 🤖 131 assistant)\n",
      "   🪙 552,066 tokens (in: 523,638, out: 28,428)\n",
      "\n",
      "7. Set up Stripe to accept donations \n",
      "   💬 140 messages (👤 19 user | 🤖 121 assistant)\n",
      "   🪙 1,333,496 tokens (in: 1,290,096, out: 43,400)\n",
      "\n",
      "8. I'm looking to see where the Cursor application stores past chats - can you help me look into where ...\n",
      "   💬 139 messages (👤 35 user | 🤖 104 assistant)\n",
      "   🪙 1,939,319 tokens (in: 1,846,083, out: 93,236)\n",
      "\n",
      "9. Let's deploy the website on fly.io - the CLI should already be connected to my account - we'll need ...\n",
      "   💬 134 messages (👤 18 user | 🤖 116 assistant)\n",
      "   🪙 1,532,515 tokens (in: 1,444,473, out: 88,042)\n",
      "\n",
      "10. Let's see how we can make our edit tab mobile friendly - this one will be hard since there's a lot o...\n",
      "   💬 132 messages (👤 16 user | 🤖 116 assistant)\n",
      "   🪙 1,389,819 tokens (in: 1,331,499, out: 58,320)\n",
      "\n",
      "11. Which tools / functions does my email service have?\n",
      "   💬 117 messages (👤 18 user | 🤖 99 assistant)\n",
      "   🪙 1,534,243 tokens (in: 1,482,429, out: 51,814)\n",
      "\n",
      "12. Each color has a unique part id - let's make an array of bricks that have the element id, num colors...\n",
      "   💬 106 messages (👤 16 user | 🤖 90 assistant)\n",
      "   🪙 940,933 tokens (in: 918,313, out: 22,620)\n",
      "\n",
      "13. I am seeing Uncaught SyntaxError: The requested module '/src/types/profile.ts' does not provide an e...\n",
      "   💬 90 messages (👤 13 user | 🤖 77 assistant)\n",
      "   🪙 1,209,419 tokens (in: 1,151,791, out: 57,628)\n",
      "\n",
      "14. Let's set up Posthog - API key is phc_bXbp6zh2R1IABqRiYNGFi9J2MiWEGmnUObNcdoS8eCL\n",
      "   💬 89 messages (👤 10 user | 🤖 79 assistant)\n",
      "   🪙 440,372 tokens (in: 428,269, out: 12,103)\n",
      "\n",
      "15. Brickit is a web app that allows folks to upload a picture and then it will be converted into a \"leg...\n",
      "   💬 84 messages (👤 7 user | 🤖 77 assistant)\n",
      "   🪙 327,489 tokens (in: 287,970, out: 39,519)\n",
      "\n",
      "16. I'd like to create a React app, using Vite, Tailwind v4, Typescript, Express \n",
      "\n",
      "The UI library should...\n",
      "   💬 81 messages (👤 12 user | 🤖 69 assistant)\n",
      "   🪙 969,935 tokens (in: 930,591, out: 39,344)\n",
      "\n",
      "17. This is for the Jamacain AI Group - but there are no colors or hints that it's Jamaica - I don't wan...\n",
      "   💬 79 messages (👤 9 user | 🤖 70 assistant)\n",
      "   🪙 765,914 tokens (in: 712,902, out: 53,012)\n",
      "\n",
      "18. In the `website` dir - Create a React app\n",
      "\n",
      "FE: Vite, Shadcn, Tailwind, Lucide\n",
      "BE: Node, Express\n",
      "\n",
      "It ...\n",
      "   💬 78 messages (👤 12 user | 🤖 66 assistant)\n",
      "   🪙 516,638 tokens (in: 469,506, out: 47,132)\n",
      "\n",
      "19. Let's make an \"admin\" page that allows us to view which users are in the platform and make edits\n",
      "   💬 76 messages (👤 14 user | 🤖 62 assistant)\n",
      "   🪙 978,990 tokens (in: 931,678, out: 47,312)\n",
      "\n",
      "20. I have the Supabase CLI connected.\n",
      "\n",
      "I added the following to my .env file \n",
      "\n",
      "Let's support a sign up ...\n",
      "   💬 73 messages (👤 16 user | 🤖 57 assistant)\n",
      "   🪙 931,522 tokens (in: 869,817, out: 61,705)\n"
     ]
    }
   ],
   "source": [
    "# Preview top conversations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 20 CONVERSATIONS (BY MESSAGE COUNT)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, conv in enumerate(export_data[\"conversations\"][:20], 1):\n",
    "    tokens = conv['tokens']\n",
    "    total_str = f\"{tokens['total']:,}\" if tokens['total'] > 0 else \"0\"\n",
    "    \n",
    "    # Build token breakdown\n",
    "    breakdown = []\n",
    "    if tokens.get('input', 0) > 0:\n",
    "        breakdown.append(f\"in: {tokens['input']:,}\")\n",
    "    if tokens.get('output', 0) > 0:\n",
    "        breakdown.append(f\"out: {tokens['output']:,}\")\n",
    "    if tokens.get('thinking', 0) > 0:\n",
    "        breakdown.append(f\"think: {tokens['thinking']:,}\")\n",
    "    \n",
    "    breakdown_str = f\" ({', '.join(breakdown)})\" if breakdown else \"\"\n",
    "    \n",
    "    print(f\"\\n{i}. {conv['title']}\")\n",
    "    print(f\"   💬 {conv['message_count']} messages (👤 {conv['user_messages']} user | 🤖 {conv['assistant_messages']} assistant)\")\n",
    "    print(f\"   🪙 {total_str} tokens{breakdown_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T20:52:21.322520Z",
     "iopub.status.busy": "2025-10-30T20:52:21.322468Z",
     "iopub.status.idle": "2025-10-30T20:52:21.357730Z",
     "shell.execute_reply": "2025-10-30T20:52:21.357403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "✅ EXPORT COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "📄 File: cursor_conversations_export.json\n",
      "📦 Size: 2.72 MB\n",
      "\n",
      "💡 You can now share this JSON file with friends!\n",
      "   It contains all your conversations in a clean, readable format.\n"
     ]
    }
   ],
   "source": [
    "# Export to JSON file\n",
    "output_file = 'cursor_conversations_export.json'\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Get file size\n",
    "file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ EXPORT COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n📄 File: {output_file}\")\n",
    "print(f\"📦 Size: {file_size_mb:.2f} MB\")\n",
    "print(f\"\\n💡 You can now share this JSON file with friends!\")\n",
    "print(f\"   It contains all your conversations in a clean, readable format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T20:52:21.358728Z",
     "iopub.status.busy": "2025-10-30T20:52:21.358663Z",
     "iopub.status.idle": "2025-10-30T20:52:21.361078Z",
     "shell.execute_reply": "2025-10-30T20:52:21.360734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXAMPLE: Viewing the most active conversation\n",
      "================================================================================\n",
      "\n",
      "Title: I made an .env file - let's set up a basic use-case for Supabase auth \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Messages: 198\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "[1] 👤 USER\n",
      "I made an .env file - let's set up a basic use-case for Supabase auth \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "[2] 🤖 ASSISTANT\n",
      "I'll help you set up Supabase authentication for your project. Let me first check your current setup and then create a basic authentication implementation.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "[3] 🤖 ASSISTANT\n",
      "Great! I can see you have the Supabase configuration set up. Now let me check the current project structure and install the necessary dependencies.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "[4] 🤖 ASSISTANT\n",
      "Now let me install the Supabase client libraries for both frontend and backend:\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "[5] 🤖 ASSISTANT\n",
      "Now let me set up the backend authentication. First, let me check the current backend structure:\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "[6] 🤖 ASSISTANT\n",
      "Now I'll create a Supabase client configuration and authentication middleware for the backend:\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "[7] 🤖 ASSISTANT\n",
      "Now let me update the main backend file to include authentication routes:\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "[8] 🤖 ASSISTANT\n",
      "Now let me set up the frontend authentication. First, let me create a Supabase client configuration for the frontend:\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "[9] 🤖 ASSISTANT\n",
      "Now let me create an authentication context and hook for the frontend:\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "[10] 🤖 ASSISTANT\n",
      "Now let me create the authentication components. First, let me create a login component:\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "... and 188 more messages\n"
     ]
    }
   ],
   "source": [
    "# Example: View a specific conversation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE: Viewing the most active conversation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if export_data[\"conversations\"]:\n",
    "    conv = export_data[\"conversations\"][0]\n",
    "    print(f\"\\nTitle: {conv['title']}\")\n",
    "    print(f\"Messages: {conv['message_count']}\\n\")\n",
    "    print(\"─\" * 80)\n",
    "    \n",
    "    # Show first 10 messages\n",
    "    for i, msg in enumerate(conv['messages'][:10], 1):\n",
    "        role_label = \"👤 USER\" if msg['role'] == 'user' else \"🤖 ASSISTANT\"\n",
    "        print(f\"\\n[{i}] {role_label}\")\n",
    "        print(msg['text'][:400])\n",
    "        if len(msg['text']) > 400:\n",
    "            print(\"...\")\n",
    "        print(\"─\" * 80)\n",
    "    \n",
    "    if conv['message_count'] > 10:\n",
    "        print(f\"\\n... and {conv['message_count'] - 10} more messages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T20:52:21.361908Z",
     "iopub.status.busy": "2025-10-30T20:52:21.361852Z",
     "iopub.status.idle": "2025-10-30T20:52:21.363451Z",
     "shell.execute_reply": "2025-10-30T20:52:21.363113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Database connection closed\n"
     ]
    }
   ],
   "source": [
    "# Close database connection\n",
    "conn.close()\n",
    "print(\"\\n✅ Database connection closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T20:52:21.364282Z",
     "iopub.status.busy": "2025-10-30T20:52:21.364230Z",
     "iopub.status.idle": "2025-10-30T20:52:21.399578Z",
     "shell.execute_reply": "2025-10-30T20:52:21.399204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "IDENTIFYING BRICKITV2 WORKSPACE IDs FROM COMPOSER DATA\n",
      "================================================================================\n",
      "\n",
      "🔍 Loading composerData entries (conversation-level)...\n",
      "✅ Loaded 189 composer entries\n",
      "\n",
      "🔍 Scanning for workspace path: /Users/shaun/Documents/GitHub/BrickitV2\n",
      "\n",
      "✅ Found 93 unique BrickitV2 workspace IDs\n",
      "📋 Workspace IDs:\n",
      "   • 00dc9230-947d-4669-a814-97ee315c8782\n",
      "   • 022b171c-f246-4718-8034-ed4cd9beee49\n",
      "   • 0445ad16-72d3-47d6-96db-6976f650c343\n",
      "   • 04b10a89-bb8b-4b0d-999a-118f3db09df5\n",
      "   • 0a37e716-7157-4e5f-9fd3-6034106fda30\n",
      "   • 0d5801ae-52c6-470b-a1e6-347ec5b4daca\n",
      "   • 0dfcd1ac-00d9-4e03-9487-0611e2e23b32\n",
      "   • 0f03be9b-9d48-45b1-b3b9-dfe33c11de5e\n",
      "   • 0f5ddb9f-47e0-410c-b439-e7ecf53c82fd\n",
      "   • 19547a45-6080-438c-8ae4-5c04d0e35fbb\n",
      "   ... and 83 more\n",
      "\n",
      "================================================================================\n",
      "FILTERING CONVERSATIONS BY WORKSPACE ID\n",
      "================================================================================\n",
      "\n",
      "✅ Found 93 BrickitV2 conversations\n",
      "💬 Total messages: 2,809\n",
      "🪙 Total tokens: 24,583,195\n",
      "\n",
      "================================================================================\n",
      "TOP 10 BRICKITV2 CONVERSATIONS\n",
      "================================================================================\n",
      "\n",
      "1. In scripts/cursor_conversations_export.ipynb\n",
      "\n",
      "Let's remove any mention of price, just focus on the t...\n",
      "   💬 178 msgs | 🪙 1,853,977 tokens\n",
      "\n",
      "2. I had tried to add a couple of security policies to ensure logged in users could upload images - I t...\n",
      "   💬 161 msgs | 🪙 3,838,370 tokens\n",
      "\n",
      "3. Can you do an SEO audit and let me know what you find?\n",
      "   💬 160 msgs | 🪙 1,139,318 tokens\n",
      "\n",
      "4. Set up Stripe to accept donations \n",
      "   💬 140 msgs | 🪙 1,333,496 tokens\n",
      "\n",
      "5. I'm looking to see where the Cursor application stores past chats - can you help me look into where ...\n",
      "   💬 139 msgs | 🪙 1,939,319 tokens\n",
      "\n",
      "6. Let's see how we can make our edit tab mobile friendly - this one will be hard since there's a lot o...\n",
      "   💬 132 msgs | 🪙 1,389,819 tokens\n",
      "\n",
      "7. Each color has a unique part id - let's make an array of bricks that have the element id, num colors...\n",
      "   💬 106 msgs | 🪙 940,933 tokens\n",
      "\n",
      "8. Let's set up Posthog - API key is phc_bXbp6zh2R1IABqRiYNGFi9J2MiWEGmnUObNcdoS8eCL\n",
      "   💬 89 msgs | 🪙 440,372 tokens\n",
      "\n",
      "9. Brickit is a web app that allows folks to upload a picture and then it will be converted into a \"leg...\n",
      "   💬 84 msgs | 🪙 327,489 tokens\n",
      "\n",
      "10. Let's make a stand-alone page to show creations that are shared - we can have the URL be /creations/...\n",
      "   💬 60 msgs | 🪙 824,320 tokens\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify BrickitV2 workspace IDs from composerData (conversation-level metadata)\n",
    "# composerData entries consistently contain workspace paths\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"IDENTIFYING BRICKITV2 WORKSPACE IDs FROM COMPOSER DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "BRICKITV2_PATH = \"/Users/shaun/Documents/GitHub/BrickitV2\"\n",
    "brickit_workspace_ids = set()\n",
    "\n",
    "# Reconnect to database to load composerData entries\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Load composerData entries to find workspace IDs with BrickitV2 path\n",
    "print(f\"\\n🔍 Loading composerData entries (conversation-level)...\")\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT key, value \n",
    "    FROM cursorDiskKV \n",
    "    WHERE key LIKE 'composerData:%'\n",
    "\"\"\")\n",
    "\n",
    "composer_entries = cursor.fetchall()\n",
    "print(f\"✅ Loaded {len(composer_entries):,} composer entries\")\n",
    "\n",
    "print(f\"\\n🔍 Scanning for workspace path: {BRICKITV2_PATH}\\n\")\n",
    "\n",
    "for key, value_blob in composer_entries:\n",
    "    try:\n",
    "        workspace_id = key.split(':')[1] if ':' in key else None\n",
    "        \n",
    "        if not workspace_id:\n",
    "            continue\n",
    "        \n",
    "        # composerData is stored as JSON blob\n",
    "        value_str = value_blob.decode('utf-8') if isinstance(value_blob, bytes) else value_blob\n",
    "        \n",
    "        # Check if BrickitV2 path is anywhere in the composer data\n",
    "        if BRICKITV2_PATH in value_str:\n",
    "            brickit_workspace_ids.add(workspace_id)\n",
    "                \n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(f\"✅ Found {len(brickit_workspace_ids)} unique BrickitV2 workspace IDs\")\n",
    "if len(brickit_workspace_ids) > 0:\n",
    "    print(f\"📋 Workspace IDs:\")\n",
    "    for wid in sorted(list(brickit_workspace_ids))[:10]:  # Show first 10\n",
    "        print(f\"   • {wid}\")\n",
    "    if len(brickit_workspace_ids) > 10:\n",
    "        print(f\"   ... and {len(brickit_workspace_ids) - 10} more\")\n",
    "\n",
    "# Step 2: Filter conversations by workspace ID\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"FILTERING CONVERSATIONS BY WORKSPACE ID\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "brickit_conversations = []\n",
    "\n",
    "for conv in export_data[\"conversations\"]:\n",
    "    if conv['workspace_id'] in brickit_workspace_ids:\n",
    "        brickit_conversations.append(conv)\n",
    "\n",
    "print(f\"\\n✅ Found {len(brickit_conversations)} BrickitV2 conversations\")\n",
    "print(f\"💬 Total messages: {sum(c['message_count'] for c in brickit_conversations):,}\")\n",
    "print(f\"🪙 Total tokens: {sum(c['tokens']['total'] for c in brickit_conversations):,}\")\n",
    "\n",
    "# Show top 10 by message count\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"TOP 10 BRICKITV2 CONVERSATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sorted_brickit = sorted(brickit_conversations, key=lambda x: x['message_count'], reverse=True)\n",
    "for i, conv in enumerate(sorted_brickit[:10], 1):\n",
    "    print(f\"\\n{i}. {conv['title']}\")\n",
    "    print(f\"   💬 {conv['message_count']} msgs | 🪙 {conv['tokens']['total']:,} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T20:52:21.400628Z",
     "iopub.status.busy": "2025-10-30T20:52:21.400565Z",
     "iopub.status.idle": "2025-10-30T20:52:21.481422Z",
     "shell.execute_reply": "2025-10-30T20:52:21.480308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SCRUBBING SECRETS FROM CONVERSATIONS\n",
      "================================================================================\n",
      "\n",
      "✅ Scrubbed 8 potential secrets\n",
      "📊 Processed 93 conversations\n",
      "\n",
      "================================================================================\n",
      "✅ BRICKITV2 EXPORT COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "📄 File: brickit_conversations_public.json\n",
      "📦 Size: 1.45 MB\n",
      "🔒 Secrets scrubbed: 8\n",
      "💬 Conversations: 93\n",
      "🪙 Total tokens: 24,583,195\n",
      "   • Input: 23,231,059\n",
      "   • Output: 1,352,136\n",
      "   • Thinking: 0\n",
      "\n",
      "✅ This file is safe to publish publicly!\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Scrub secrets from BrickitV2 conversations\n",
    "import re\n",
    "from detect_secrets import SecretsCollection\n",
    "from detect_secrets.settings import default_settings\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SCRUBBING SECRETS FROM CONVERSATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Common patterns for secrets\n",
    "secret_patterns = [\n",
    "    (r'sk-(?:proj-)?[a-zA-Z0-9_-]{20,}', '[OPENAI_API_KEY]'),  # OpenAI keys (including sk-proj-)\n",
    "    (r'phc_[a-zA-Z0-9]{32,}', '[POSTHOG_API_KEY]'),  # PostHog keys  \n",
    "    (r'pk_live_[a-zA-Z0-9]{24,}', '[STRIPE_PUBLIC_KEY]'),  # Stripe public\n",
    "    (r'sk_live_[a-zA-Z0-9]{24,}', '[STRIPE_SECRET_KEY]'),  # Stripe secret\n",
    "    (r'pk_test_[a-zA-Z0-9]{24,}', '[STRIPE_TEST_PUBLIC_KEY]'),  # Stripe test\n",
    "    (r'sk_test_[a-zA-Z0-9]{24,}', '[STRIPE_TEST_SECRET_KEY]'),  # Stripe test\n",
    "    (r'eyJ[a-zA-Z0-9_-]{10,}\\.[a-zA-Z0-9_-]{10,}\\.[a-zA-Z0-9_-]{10,}', '[JWT_TOKEN]'),  # JWT\n",
    "    (r'[a-zA-Z0-9]{32,64}(?=\\s*[\"\\']?\\s*(?:api|secret|key|token|password))', '[API_KEY]'),  # Generic keys\n",
    "    (r'ghp_[a-zA-Z0-9]{36,}', '[GITHUB_PAT]'),  # GitHub Personal Access Token\n",
    "    (r'gho_[a-zA-Z0-9]{36,}', '[GITHUB_OAUTH]'),  # GitHub OAuth\n",
    "    (r'AIza[0-9A-Za-z\\\\-_]{35}', '[GOOGLE_API_KEY]'),  # Google API\n",
    "]\n",
    "\n",
    "def scrub_text(text):\n",
    "    \"\"\"Remove API keys and secrets from text\"\"\"\n",
    "    scrubbed = text\n",
    "    replacements_made = []\n",
    "    \n",
    "    for pattern, replacement in secret_patterns:\n",
    "        matches = re.findall(pattern, scrubbed)\n",
    "        if matches:\n",
    "            replacements_made.extend([(m[:10] + '...', replacement) for m in matches])\n",
    "            scrubbed = re.sub(pattern, replacement, scrubbed)\n",
    "    \n",
    "    return scrubbed, replacements_made\n",
    "\n",
    "# Scrub all BrickitV2 conversations\n",
    "scrubbed_conversations = []\n",
    "total_secrets_found = 0\n",
    "\n",
    "for conv in brickit_conversations:\n",
    "    # Scrub the title as well\n",
    "    scrubbed_title, title_replacements = scrub_text(conv['title'])\n",
    "    total_secrets_found += len(title_replacements)\n",
    "    \n",
    "    scrubbed_conv = {\n",
    "        **conv,\n",
    "        'title': scrubbed_title,\n",
    "        'messages': []\n",
    "    }\n",
    "    \n",
    "    for msg in conv['messages']:\n",
    "        scrubbed_text, replacements = scrub_text(msg['text'])\n",
    "        total_secrets_found += len(replacements)\n",
    "        \n",
    "        scrubbed_msg = {\n",
    "            **msg,\n",
    "            'text': scrubbed_text\n",
    "        }\n",
    "        scrubbed_conv['messages'].append(scrubbed_msg)\n",
    "    \n",
    "    scrubbed_conversations.append(scrubbed_conv)\n",
    "\n",
    "print(f\"\\n✅ Scrubbed {total_secrets_found} potential secrets\")\n",
    "print(f\"📊 Processed {len(scrubbed_conversations)} conversations\")\n",
    "\n",
    "# Create export\n",
    "brickit_export = {\n",
    "    \"metadata\": {\n",
    "        \"export_date\": datetime.now().isoformat(),\n",
    "        \"project\": \"BrickitV2\",\n",
    "        \"project_path\": \"/Users/shaun/Documents/GitHub/BrickitV2\",\n",
    "        \"total_conversations\": len(scrubbed_conversations),\n",
    "        \"total_messages\": sum(c['message_count'] for c in scrubbed_conversations),\n",
    "        \"total_tokens\": {\n",
    "            \"input\": sum(c['tokens']['input'] for c in scrubbed_conversations),\n",
    "            \"output\": sum(c['tokens']['output'] for c in scrubbed_conversations),\n",
    "            \"thinking\": sum(c['tokens'].get('thinking', 0) for c in scrubbed_conversations),\n",
    "            \"total\": sum(c['tokens']['total'] for c in scrubbed_conversations)\n",
    "        },\n",
    "        \"secrets_scrubbed\": total_secrets_found,\n",
    "        \"scrubbing_note\": \"API keys, tokens, and secrets have been automatically redacted from messages and titles\"\n",
    "    },\n",
    "    \"conversations\": scrubbed_conversations\n",
    "}\n",
    "\n",
    "# Sort by message count\n",
    "brickit_export[\"conversations\"].sort(key=lambda x: x['message_count'], reverse=True)\n",
    "\n",
    "# Export to file\n",
    "output_file = 'brickit_conversations_public.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(brickit_export, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"✅ BRICKITV2 EXPORT COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n📄 File: {output_file}\")\n",
    "print(f\"📦 Size: {file_size_mb:.2f} MB\")\n",
    "print(f\"🔒 Secrets scrubbed: {total_secrets_found}\")\n",
    "print(f\"💬 Conversations: {len(scrubbed_conversations)}\")\n",
    "print(f\"🪙 Total tokens: {brickit_export['metadata']['total_tokens']['total']:,}\")\n",
    "print(f\"   • Input: {brickit_export['metadata']['total_tokens']['input']:,}\")\n",
    "print(f\"   • Output: {brickit_export['metadata']['total_tokens']['output']:,}\")\n",
    "print(f\"   • Thinking: {brickit_export['metadata']['total_tokens']['thinking']:,}\")\n",
    "print(f\"\\n✅ This file is safe to publish publicly!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
